{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-02 12:33:59.869829: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-08-02 12:33:59.869871: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "/accounts/projects/vision/.local/lib/python3.8/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n",
      "2022-08-02 12:34:17.477852: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-08-02 12:34:17.477893: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-08-02 12:34:17.477925: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (scf-sm22): /proc/driver/nvidia/version does not exist\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from transformers import BertModel, BertConfig, DistilBertModel\n",
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import datasets\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import data\n",
    "from copy import deepcopy\n",
    "from spacy.lang.en import English\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# look at some dsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.load_dataset('imdb')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['validation'] = dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del dataset['test']\n",
    "# dataset['train'] = dataset['train'].select(range(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(dataset['test']['label']) # weird error -- all test labels seem to be -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**emotion**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.load_dataset('emotion')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['train']['label'][-200:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['train']['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**rotten_tomatoes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.load_dataset('rotten_tomatoes')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**tweet_eval**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.load_dataset('tweet_eval', 'hate')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['test']['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**financial_phrasebank**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset financial_phrasebank/sentences_75agree (download: 665.91 KiB, generated: 461.62 KiB, post-processed: Unknown size, total: 1.10 MiB) to /tmp/.xdg_cache_vision/huggingface/datasets/financial_phrasebank/sentences_75agree/1.0.0/550bde12e6c30e2674da973a55f57edde5181d53f5a5a34c1531c53f93b7e141...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/3453 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset financial_phrasebank downloaded and prepared to /tmp/.xdg_cache_vision/huggingface/datasets/financial_phrasebank/sentences_75agree/1.0.0/550bde12e6c30e2674da973a55f57edde5181d53f5a5a34c1531c53f93b7e141. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1eaba2d6af9a4d709df4b638a54f8a92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = datasets.load_dataset('financial_phrasebank', 'sentences_75agree', revision='main')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence', 'label'],\n",
       "        num_rows: 3453\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset financial_phrasebank (/tmp/.xdg_cache_vision/huggingface/datasets/financial_phrasebank/sentences_75agree/1.0.0/550bde12e6c30e2674da973a55f57edde5181d53f5a5a34c1531c53f93b7e141)\n"
     ]
    }
   ],
   "source": [
    "train = datasets.load_dataset('financial_phrasebank', 'sentences_75agree', revision='main', split='train')\n",
    "idxs_train, idxs_val = train_test_split(np.arange(len(train)), test_size=0.25)\n",
    "d = datasets.DatasetDict()\n",
    "d['train'] = train.select(idxs_train)\n",
    "d['validation'] = train.select(idxs_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2655, 2891, 3372, ..., 1145, 2170, 2066])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idxs_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['sentence', 'label'],\n",
       "    num_rows: 2\n",
       "})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.select(np.array([0, 3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# datasets overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset financial_phrasebank (/scratch/users/vision/huggingface-cache/financial_phrasebank/sentences_75agree/1.0.0/550bde12e6c30e2674da973a55f57edde5181d53f5a5a34c1531c53f93b7e141)\n"
     ]
    }
   ],
   "source": [
    "tok_simp = English().tokenizer # init here to speedup call\n",
    "simple_tokenizer = lambda x: [str(x) for x in tok_simp(x)] \n",
    "ds = defaultdict(list)\n",
    "class Args:\n",
    "    ...\n",
    "    \n",
    "args = Args()\n",
    "args.dataset = ''\n",
    "ks = sorted(['emotion', 'financial_phrasebank', 'rotten_tomatoes', 'sst2', 'tweet_eval'])\n",
    "for k in ks:\n",
    "    args.dataset = k\n",
    "    d, args = data.process_data_and_args(args)\n",
    "    text = d['train'][args.dataset_key_text]\n",
    "    ds['n_train'].append(len(text))\n",
    "    \n",
    "    \n",
    "    counts = np.unique(d['train']['label'], return_counts=True)[1]\n",
    "    ds['imbalance'].append(max(counts) / sum(counts))\n",
    "    \n",
    "    ds['num_classes'].append(counts.size)\n",
    "    \n",
    "    text_val = d['validation'][args.dataset_key_text]\n",
    "    ds['n_val'].append(len(text_val))    \n",
    "    \n",
    "    v = CountVectorizer(tokenizer=simple_tokenizer)\n",
    "    v.fit(text)\n",
    "    ds['n_tokens'].append(len(v.vocabulary_))\n",
    "    \n",
    "    v = CountVectorizer(tokenizer=simple_tokenizer, ngram_range=(2, 2))\n",
    "    v.fit(text)\n",
    "    ds['n_bigrams'].append(len(v.vocabulary_))\n",
    "    \n",
    "    v = CountVectorizer(tokenizer=simple_tokenizer, ngram_range=(3, 3))\n",
    "    v.fit(text)\n",
    "    ds['n_trigrams'].append(len(v.vocabulary_))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict(ds)\n",
    "df.index = ks\n",
    "df\n",
    "df.to_csv('results/datasets_ovw.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('results/datasets_ovw.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_train</th>\n",
       "      <th>imbalance</th>\n",
       "      <th>num_classes</th>\n",
       "      <th>n_val</th>\n",
       "      <th>n_tokens</th>\n",
       "      <th>n_bigrams</th>\n",
       "      <th>n_trigrams</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>emotion</th>\n",
       "      <td>16000</td>\n",
       "      <td>0.335125</td>\n",
       "      <td>6</td>\n",
       "      <td>2000</td>\n",
       "      <td>15165</td>\n",
       "      <td>106201</td>\n",
       "      <td>201404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>financial_phrasebank</th>\n",
       "      <td>2313</td>\n",
       "      <td>0.623433</td>\n",
       "      <td>3</td>\n",
       "      <td>1140</td>\n",
       "      <td>7169</td>\n",
       "      <td>28481</td>\n",
       "      <td>39597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rotten_tomatoes</th>\n",
       "      <td>8530</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>2</td>\n",
       "      <td>1066</td>\n",
       "      <td>16631</td>\n",
       "      <td>93921</td>\n",
       "      <td>147426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sst2</th>\n",
       "      <td>67349</td>\n",
       "      <td>0.557826</td>\n",
       "      <td>2</td>\n",
       "      <td>872</td>\n",
       "      <td>13887</td>\n",
       "      <td>72501</td>\n",
       "      <td>108800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tweet_eval</th>\n",
       "      <td>9000</td>\n",
       "      <td>0.579667</td>\n",
       "      <td>2</td>\n",
       "      <td>1000</td>\n",
       "      <td>18476</td>\n",
       "      <td>106277</td>\n",
       "      <td>171769</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      n_train  imbalance  num_classes  n_val  n_tokens  \\\n",
       "emotion                 16000   0.335125            6   2000     15165   \n",
       "financial_phrasebank     2313   0.623433            3   1140      7169   \n",
       "rotten_tomatoes          8530   0.500000            2   1066     16631   \n",
       "sst2                    67349   0.557826            2    872     13887   \n",
       "tweet_eval               9000   0.579667            2   1000     18476   \n",
       "\n",
       "                      n_bigrams  n_trigrams  \n",
       "emotion                  106201      201404  \n",
       "financial_phrasebank      28481       39597  \n",
       "rotten_tomatoes           93921      147426  \n",
       "sst2                      72501      108800  \n",
       "tweet_eval               106277      171769  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrrrrrrr}\n",
      "\\toprule\n",
      "{} &  Samples (train) &  Samples (val) &  Unigrams &  Bigrams &  Trigrams &  Classes &  Majority class fraction \\\\\n",
      "\\midrule\n",
      "Emotion              &            16000 &           2000 &     15165 &   106201 &    201404 &        6 &                     0.34 \\\\\n",
      "Financial phrasebank &             2313 &           1140 &      7169 &    28481 &     39597 &        3 &                     0.62 \\\\\n",
      "Rotten tomatoes      &             8530 &           1066 &     16631 &    93921 &    147426 &        2 &                      0.5 \\\\\n",
      "SST2                 &            67349 &            872 &     13887 &    72501 &    108800 &        2 &                     0.56 \\\\\n",
      "Tweet (Hate)         &             9000 &           1000 &     18476 &   106277 &    171769 &        2 &                     0.58 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def prep_for_printing(df):\n",
    "    df = df.sort_values('n_train')\n",
    "    df['num_classes'] = df.pop('num_classes') # move imbalance to end\n",
    "    df['imbalance'] = df.pop('imbalance') # move imbalance to end\n",
    "    df = df.infer_objects()\n",
    "    df = df.rename(\n",
    "        columns=data.COLUMNS_RENAME_DICT,\n",
    "        index=data.DSETS_RENAME_DICT,\n",
    "    ).round(2).sort_index()\n",
    "    return df\n",
    "\n",
    "pd.options.display.float_format = '{:,}'.format\n",
    "print(prep_for_printing(df).to_latex())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english)\n"
     ]
    }
   ],
   "source": [
    "# classifier = pipeline(\"sentiment-analysis\", model='bert-base-uncased')\n",
    "classifier = pipeline(\"sentiment-analysis\") #, model='bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc 0.907\n"
     ]
    }
   ],
   "source": [
    "N = 150\n",
    "split = 'validation' # weird error -- all test labels seem to be -1\n",
    "preds = classifier(dataset[split]['sentence'][:N])\n",
    "M = {'POSITIVE': 1, 'NEGATIVE': 0}\n",
    "preds = [M[p['label']] for p in preds]\n",
    "# print(preds, dataset[split]['label'])\n",
    "print('Acc', np.mean(np.array(preds) == np.array(dataset[split]['label'])[:N]).round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**feature extraction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english were not used when initializing DistilBertModel: ['classifier.weight', 'classifier.bias', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12, 10, 24, 28, 13, 25, 8, 15, 26, 40, 23, 39, 25, 26, 29, 26, 21, 44, 14, 21, 41, 17, 10, 20, 18, 35, 35, 28, 19, 13, 35, 38, 10, 39, 27, 14, 31, 40, 27, 23, 25, 30, 26, 19, 21, 18, 23, 17, 14, 25]\n"
     ]
    }
   ],
   "source": [
    "feature_extractor = pipeline(\"feature-extraction\",\n",
    "                             model='distilbert-base-uncased-finetuned-sst-2-english')\n",
    "feats_list  = feature_extractor(dataset[split]['sentence'][:N])\n",
    "print([len(feats[x][0]) for x in range(10)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, features with different length sequences have different-sized embeddings. To fix this, let's apply padding during embedding step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = BertModel.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens torch.Size([10, 29])\n",
      "embeddings (10, 768)\n"
     ]
    }
   ],
   "source": [
    "sequences = dataset['train']['sentence'][:10]\n",
    "tokens = tokenizer(sequences, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "print('tokens', tokens['input_ids'].shape)\n",
    "output = model(**tokens) # has two keys, 'last_hidden_state', 'pooler_output'\n",
    "embs = output['pooler_output'].cpu().detach().numpy()\n",
    "print('embeddings', embs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "**try another model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89d75b1afcee40cdb0e6c876a2416148",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/477 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f99c738163164828993e32481e996f48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/418M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at textattack/bert-base-uncased-SST-2 were not used when initializing BertModel: ['classifier.bias', 'classifier.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d502bb7997849d5a86ff7ecfc4ed5e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d72124810e194c3a95139265e0b053db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c530e1588135401c8cdfe6e2750bd892",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "# model = DistilBertModel.from_pretrained(checkpoint)\n",
    "checkpoint = 'textattack/bert-base-uncased-SST-2'\n",
    "model = BertModel.from_pretrained(checkpoint)\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens torch.Size([100, 51])\n"
     ]
    }
   ],
   "source": [
    "sequences = dataset['train']['sentence'][:100]\n",
    "tokens = tokenizer(sequences, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "print('tokens', tokens['input_ids'].shape)\n",
    "output = model(**tokens) # has two keys, 'last_hidden_state', 'pooler_output'\n",
    "# embs = output['pooler_output'].cpu().detach().numpy()\n",
    "# print('embeddings', embs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['last_hidden_state', 'pooler_output'])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 768])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output['pooler_output'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# custom feature extraction\n",
    "let's map each sequence to the sum of features obtained by each token (tutorial [here](https://huggingface.co/course/chapter3/2?fw=pt))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = BertModel.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featurize_function(example):\n",
    "    tokens = tokenizer(example['sentence'], padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    output = model(**tokens) # has two keys, 'last_hidden_state', 'pooler_output'\n",
    "    embs = output['pooler_output'].cpu().detach().numpy()\n",
    "    return {'embs': embs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /tmp/.xdg_cache_vision/huggingface/datasets/sst2/default/2.0.0/9896208a8d85db057ac50c72282bcb8fe755accc671a57dd8059d4e130961ed5/cache-7876086904868ef1.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['idx', 'sentence', 'label'],\n",
       "    num_rows: 8\n",
       "})"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dset_small = dataset['validation'] #[:10]\n",
    "dset_small = dset_small.filter(lambda x: len(x[\"sentence\"]) < 20)\n",
    "dset_small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "042dcb2d602347728fab7d74c91543bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "featurized_dataset = dset_small.map(featurize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 768)"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(featurized_dataset['embs']).shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
