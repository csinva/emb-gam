{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from transformers import BertModel, BertConfig, DistilBertModel\n",
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import datasets\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import data\n",
    "from copy import deepcopy\n",
    "from spacy.lang.en import English\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# look at some dsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imodelsx.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# datasets overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chansingh/imodelsx/.venv/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67a248348bf440d99a5838af20bf86f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tok_simp = English().tokenizer  # init here to speedup call\n",
    "simple_tokenizer = lambda x: [str(x) for x in tok_simp(x)]\n",
    "ds = defaultdict(list)\n",
    "# ks = sorted(['emotion', 'financial_phrasebank', 'rotten_tomatoes', 'sst2', 'tweet_eval'])\n",
    "ks = [\"financial_phrasebank\", 'emotion']\n",
    "\n",
    "for k in ks:\n",
    "    d, dataset_key_text = imodelsx.data.load_huggingface_dataset(\n",
    "        dataset_name=k\n",
    "    )\n",
    "\n",
    "    text = d[\"train\"][dataset_key_text]\n",
    "    ds[\"n_train\"].append(len(text))\n",
    "\n",
    "    counts = np.unique(d[\"train\"][\"label\"], return_counts=True)[1]\n",
    "    ds[\"imbalance\"].append(max(counts) / sum(counts))\n",
    "\n",
    "    ds[\"num_classes\"].append(counts.size)\n",
    "\n",
    "    text_val = d[\"validation\"][dataset_key_text]\n",
    "    ds[\"n_val\"].append(len(text_val))\n",
    "\n",
    "    v = CountVectorizer(tokenizer=simple_tokenizer)\n",
    "    v.fit(text)\n",
    "    ds[\"n_tokens\"].append(len(v.vocabulary_))\n",
    "    # count unseen tokens in validation set\n",
    "    ds[\"n_tokens_unseen\"].append(\n",
    "        len(\n",
    "            set(v.vocabulary_.keys())\n",
    "            - set(\n",
    "                CountVectorizer(tokenizer=simple_tokenizer)\n",
    "                .fit(text_val)\n",
    "                .vocabulary_.keys()\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "\n",
    "    v = CountVectorizer(tokenizer=simple_tokenizer, ngram_range=(2, 2))\n",
    "    v.fit(text)\n",
    "    ds[\"n_bigrams\"].append(len(v.vocabulary_))\n",
    "    # count unseen bigrams in validation set\n",
    "    ds[\"n_bigrams_unseen\"].append(\n",
    "        len(\n",
    "            set(v.vocabulary_.keys())\n",
    "            - set(\n",
    "                CountVectorizer(tokenizer=simple_tokenizer, ngram_range=(2, 2))\n",
    "                .fit(text_val)\n",
    "                .vocabulary_.keys()\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "\n",
    "    v = CountVectorizer(tokenizer=simple_tokenizer, ngram_range=(3, 3))\n",
    "    v.fit(text)\n",
    "    ds[\"n_trigrams\"].append(len(v.vocabulary_))\n",
    "    # count unseen trigrams in validation set\n",
    "    ds[\"n_trigrams_unseen\"].append(\n",
    "        len(\n",
    "            set(v.vocabulary_.keys())\n",
    "            - set(\n",
    "                CountVectorizer(tokenizer=simple_tokenizer, ngram_range=(3, 3))\n",
    "                .fit(text_val)\n",
    "                .vocabulary_.keys()\n",
    "            )\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict(ds)\n",
    "df.index = ks\n",
    "df\n",
    "df.to_csv('../../results/datasets_ovw.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../results/datasets_ovw.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>financial_phrasebank</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>n_train</th>\n",
       "      <td>2313.0</td>\n",
       "      <td>16000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>imbalance</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>num_classes</th>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n_val</th>\n",
       "      <td>1140.0</td>\n",
       "      <td>2000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n_tokens</th>\n",
       "      <td>7169.0</td>\n",
       "      <td>15165.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n_tokens_unseen</th>\n",
       "      <td>4260.0</td>\n",
       "      <td>11384.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n_bigrams</th>\n",
       "      <td>28481.0</td>\n",
       "      <td>106201.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n_bigrams_unseen</th>\n",
       "      <td>22878.0</td>\n",
       "      <td>95466.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n_trigrams</th>\n",
       "      <td>39597.0</td>\n",
       "      <td>201404.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n_trigrams_unseen</th>\n",
       "      <td>35920.0</td>\n",
       "      <td>193555.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   financial_phrasebank   emotion\n",
       "n_train                          2313.0   16000.0\n",
       "imbalance                           1.0       0.0\n",
       "num_classes                         3.0       6.0\n",
       "n_val                            1140.0    2000.0\n",
       "n_tokens                         7169.0   15165.0\n",
       "n_tokens_unseen                  4260.0   11384.0\n",
       "n_bigrams                       28481.0  106201.0\n",
       "n_bigrams_unseen                22878.0   95466.0\n",
       "n_trigrams                      39597.0  201404.0\n",
       "n_trigrams_unseen               35920.0  193555.0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.T.round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in ['n_tokens', 'n_bigrams', 'n_trigrams']:\n",
    "    df[k + '_tot'] = df[k] + df[k + '_unseen']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>financial_phrasebank</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>n_train</th>\n",
       "      <td>2313.0</td>\n",
       "      <td>16000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>imbalance</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>num_classes</th>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n_val</th>\n",
       "      <td>1140.0</td>\n",
       "      <td>2000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n_tokens</th>\n",
       "      <td>7169.0</td>\n",
       "      <td>15165.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n_tokens_unseen</th>\n",
       "      <td>4260.0</td>\n",
       "      <td>11384.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n_bigrams</th>\n",
       "      <td>28481.0</td>\n",
       "      <td>106201.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n_bigrams_unseen</th>\n",
       "      <td>22878.0</td>\n",
       "      <td>95466.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n_trigrams</th>\n",
       "      <td>39597.0</td>\n",
       "      <td>201404.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n_trigrams_unseen</th>\n",
       "      <td>35920.0</td>\n",
       "      <td>193555.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n_tokens_tot</th>\n",
       "      <td>11429.0</td>\n",
       "      <td>26549.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n_bigrams_tot</th>\n",
       "      <td>51359.0</td>\n",
       "      <td>201667.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n_trigrams_tot</th>\n",
       "      <td>75517.0</td>\n",
       "      <td>394959.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   financial_phrasebank   emotion\n",
       "n_train                          2313.0   16000.0\n",
       "imbalance                           1.0       0.0\n",
       "num_classes                         3.0       6.0\n",
       "n_val                            1140.0    2000.0\n",
       "n_tokens                         7169.0   15165.0\n",
       "n_tokens_unseen                  4260.0   11384.0\n",
       "n_bigrams                       28481.0  106201.0\n",
       "n_bigrams_unseen                22878.0   95466.0\n",
       "n_trigrams                      39597.0  201404.0\n",
       "n_trigrams_unseen               35920.0  193555.0\n",
       "n_tokens_tot                    11429.0   26549.0\n",
       "n_bigrams_tot                   51359.0  201667.0\n",
       "n_trigrams_tot                  75517.0  394959.0"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.T.round(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrrrrrrr}\n",
      "\\toprule\n",
      "{} &  Samples (train) &  Samples (val) &  Unigrams &  Bigrams &  Trigrams &  Classes &  Majority class fraction \\\\\n",
      "\\midrule\n",
      "Emotion              &            16000 &           2000 &     15165 &   106201 &    201404 &        6 &                     0.34 \\\\\n",
      "Financial phrasebank &             2313 &           1140 &      7169 &    28481 &     39597 &        3 &                     0.62 \\\\\n",
      "Rotten tomatoes      &             8530 &           1066 &     16631 &    93921 &    147426 &        2 &                      0.5 \\\\\n",
      "SST2                 &            67349 &            872 &     13887 &    72501 &    108800 &        2 &                     0.56 \\\\\n",
      "Tweet (Hate)         &             9000 &           1000 &     18476 &   106277 &    171769 &        2 &                     0.58 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def prep_for_printing(df):\n",
    "    df = df.sort_values('n_train')\n",
    "    df['num_classes'] = df.pop('num_classes') # move imbalance to end\n",
    "    df['imbalance'] = df.pop('imbalance') # move imbalance to end\n",
    "    df = df.infer_objects()\n",
    "    df = df.rename(\n",
    "        columns=data.COLUMNS_RENAME_DICT,\n",
    "        index=data.DSETS_RENAME_DICT,\n",
    "    ).round(2).sort_index()\n",
    "    return df\n",
    "\n",
    "pd.options.display.float_format = '{:,}'.format\n",
    "print(prep_for_printing(df).to_latex())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english)\n"
     ]
    }
   ],
   "source": [
    "# classifier = pipeline(\"sentiment-analysis\", model='bert-base-uncased')\n",
    "classifier = pipeline(\"sentiment-analysis\") #, model='bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc 0.907\n"
     ]
    }
   ],
   "source": [
    "N = 150\n",
    "split = 'validation' # weird error -- all test labels seem to be -1\n",
    "preds = classifier(dataset[split]['sentence'][:N])\n",
    "M = {'POSITIVE': 1, 'NEGATIVE': 0}\n",
    "preds = [M[p['label']] for p in preds]\n",
    "# print(preds, dataset[split]['label'])\n",
    "print('Acc', np.mean(np.array(preds) == np.array(dataset[split]['label'])[:N]).round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**feature extraction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english were not used when initializing DistilBertModel: ['classifier.weight', 'classifier.bias', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12, 10, 24, 28, 13, 25, 8, 15, 26, 40, 23, 39, 25, 26, 29, 26, 21, 44, 14, 21, 41, 17, 10, 20, 18, 35, 35, 28, 19, 13, 35, 38, 10, 39, 27, 14, 31, 40, 27, 23, 25, 30, 26, 19, 21, 18, 23, 17, 14, 25]\n"
     ]
    }
   ],
   "source": [
    "feature_extractor = pipeline(\"feature-extraction\",\n",
    "                             model='distilbert-base-uncased-finetuned-sst-2-english')\n",
    "feats_list  = feature_extractor(dataset[split]['sentence'][:N])\n",
    "print([len(feats[x][0]) for x in range(10)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, features with different length sequences have different-sized embeddings. To fix this, let's apply padding during embedding step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = BertModel.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens torch.Size([10, 29])\n",
      "embeddings (10, 768)\n"
     ]
    }
   ],
   "source": [
    "sequences = dataset['train']['sentence'][:10]\n",
    "tokens = tokenizer(sequences, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "print('tokens', tokens['input_ids'].shape)\n",
    "output = model(**tokens) # has two keys, 'last_hidden_state', 'pooler_output'\n",
    "embs = output['pooler_output'].cpu().detach().numpy()\n",
    "print('embeddings', embs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "**try another model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89d75b1afcee40cdb0e6c876a2416148",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/477 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f99c738163164828993e32481e996f48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/418M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at textattack/bert-base-uncased-SST-2 were not used when initializing BertModel: ['classifier.bias', 'classifier.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d502bb7997849d5a86ff7ecfc4ed5e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d72124810e194c3a95139265e0b053db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c530e1588135401c8cdfe6e2750bd892",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "# model = DistilBertModel.from_pretrained(checkpoint)\n",
    "checkpoint = 'textattack/bert-base-uncased-SST-2'\n",
    "model = BertModel.from_pretrained(checkpoint)\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens torch.Size([100, 51])\n"
     ]
    }
   ],
   "source": [
    "sequences = dataset['train']['sentence'][:100]\n",
    "tokens = tokenizer(sequences, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "print('tokens', tokens['input_ids'].shape)\n",
    "output = model(**tokens) # has two keys, 'last_hidden_state', 'pooler_output'\n",
    "# embs = output['pooler_output'].cpu().detach().numpy()\n",
    "# print('embeddings', embs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['last_hidden_state', 'pooler_output'])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 768])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output['pooler_output'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# custom feature extraction\n",
    "let's map each sequence to the sum of features obtained by each token (tutorial [here](https://huggingface.co/course/chapter3/2?fw=pt))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = BertModel.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featurize_function(example):\n",
    "    tokens = tokenizer(example['sentence'], padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    output = model(**tokens) # has two keys, 'last_hidden_state', 'pooler_output'\n",
    "    embs = output['pooler_output'].cpu().detach().numpy()\n",
    "    return {'embs': embs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /tmp/.xdg_cache_vision/huggingface/datasets/sst2/default/2.0.0/9896208a8d85db057ac50c72282bcb8fe755accc671a57dd8059d4e130961ed5/cache-7876086904868ef1.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['idx', 'sentence', 'label'],\n",
       "    num_rows: 8\n",
       "})"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dset_small = dataset['validation'] #[:10]\n",
    "dset_small = dset_small.filter(lambda x: len(x[\"sentence\"]) < 20)\n",
    "dset_small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "042dcb2d602347728fab7d74c91543bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "featurized_dataset = dset_small.map(featurize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 768)"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(featurized_dataset['embs']).shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
