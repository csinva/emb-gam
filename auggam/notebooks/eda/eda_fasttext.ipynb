{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# convert data to fasttext format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from transformers import BertModel, DistilBertModel\n",
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import datasets\n",
    "import numpy as np\n",
    "from datasets import load_from_disk\n",
    "import pickle as pkl\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from collections import defaultdict\n",
    "import os\n",
    "from os.path import join as oj\n",
    "from copy import deepcopy\n",
    "import fasttext\n",
    "import config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prepare data for fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset sst2 (/tmp/.xdg_cache_vision/huggingface/datasets/sst2/default/2.0.0/9896208a8d85db057ac50c72282bcb8fe755accc671a57dd8059d4e130961ed5)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61bc11d8defd41f29ed5d23382c5ee6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['idx', 'sentence', 'label'],\n",
       "        num_rows: 67349\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['idx', 'sentence', 'label'],\n",
       "        num_rows: 872\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['idx', 'sentence', 'label'],\n",
       "        num_rows: 1821\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = datasets.load_dataset('sst2')\n",
    "for split in ['train', 'validation']:\n",
    "    for subsample in [100, 1000, -1]:\n",
    "        d = dataset[split]\n",
    "        if subsample and split == 'train':\n",
    "            d = d[:subsample]\n",
    "        vals = ('__label__' + pd.Series(d['label']).astype(str) + ' ' + pd.Series(d['sentence']).astype(str)).values\n",
    "        s = '\\n'.join(vals)\n",
    "        with open(f'data/sst2-fasttext/{split}_sst_{subsample}_sst.txt', 'w') as f:\n",
    "            f.write(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fit fasttext model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred(x):\n",
    "    return 1 * ('1' in m.predict(x)[0][0])\n",
    "\n",
    "\n",
    "for subsample in tqdm([100, 1000, -1]):\n",
    "    for ngrams in range(1, 10):\n",
    "        r = {\n",
    "            'checkpoint': 'fasttext',\n",
    "            'ngrams': ngrams,\n",
    "            'subsample': subsample,\n",
    "            'all': 'all',\n",
    "        }\n",
    "\n",
    "        # saving\n",
    "        results_dir = oj(config.results_dir, 'sst2')\n",
    "        dir_name = f\"ngram={ngrams}_\" + 'sub=' + str(subsample) + '_' + r['checkpoint'] + '-all'\n",
    "        save_dir = oj(results_dir, dir_name)\n",
    "        if os.path.exists(save_dir):\n",
    "            print('aready ran', save_dir)\n",
    "\n",
    "\n",
    "        m = fasttext.train_supervised(f'data/sst2-fasttext/train_sst_{subsample}_sst.txt',\n",
    "                                      wordNgrams=ngrams)\n",
    "        # m.test('data/sst2-fasttext/train_sst_100_sst.txt')\n",
    "        preds = np.array(list(map(pred, dataset['train']['sentence']))).astype(int)\n",
    "        labels = np.array(dataset['train']['label']).astype(int)\n",
    "        r['acc_train'] = np.mean(preds == labels)\n",
    "        \n",
    "        preds = np.array(list(map(pred, dataset['validation']['sentence']))).astype(int)\n",
    "        labels = np.array(dataset['validation']['label']).astype(int)\n",
    "        r['acc_val'] = np.mean(preds == labels)\n",
    "#         r['model'] = m\n",
    "        r['num_features'] = len(m.words)\n",
    "        \n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        with open(oj(save_dir, 'results.pkl'), 'wb') as f:\n",
    "            pkl.dump(r, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
