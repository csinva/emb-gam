{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from os.path import join\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "# import notebook_helper\n",
    "import imodelsx.process_results\n",
    "from sklearn.tree import plot_tree\n",
    "import sys\n",
    "import numpy as np\n",
    "import llm_tree.data\n",
    "import dvu\n",
    "import viz\n",
    "import scipy.stats\n",
    "import warnings\n",
    "dvu.set_style()\n",
    "plt.rcParams['font.size'] = '14'\n",
    "sys.path.append('../experiments/')\n",
    "results_dir = '../results/feb11/'\n",
    "\n",
    "# load results as dataframe\n",
    "r = imodelsx.process_results.get_results_df(results_dir, use_cached=True)\n",
    "\n",
    "# fill missing args with default values from argparse\n",
    "experiment_filename = '../experiments/01_train_model.py'\n",
    "r = imodelsx.process_results.fill_missing_args_with_default(r, experiment_filename)\n",
    "r = r[~(r.model_name == 'hstree')]\n",
    "r = r[~(r.model_name == 'linear_finetune')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average over random seeds\n",
    "ravg = imodelsx.process_results.average_over_seeds(r, experiment_filename)\n",
    "\n",
    "default_params = {\n",
    "    'max_features': 1,\n",
    "    'ngrams': 2,\n",
    "    'refinement_strategy': 'llm',\n",
    "    'use_llm_prompt_context': 0,\n",
    "    'use_stemming': 0,\n",
    "    # 'n_estimators': 1,\n",
    "    # 'subsample_frac': 1,\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single-tree curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rcurve = ravg[ravg.n_estimators == 1] # exclude ensembles\n",
    "rcurve = rcurve[rcurve.subsample_frac == 1] # exclude subsampling\n",
    "\n",
    "\n",
    "groupings = ['model_name', 'max_features', 'ngrams',\n",
    "             'refinement_strategy', 'use_llm_prompt_context', 'use_stemming']\n",
    "metric = 'roc_auc_test'\n",
    "# metric = 'accuracy_test'\n",
    "\n",
    "# viz.plot_train_and_test(rcurve, groupings, metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rp = rcurve\n",
    "for k in default_params:\n",
    "    rp = rp[rp[k] == default_params[k]]\n",
    "viz.plot_curves(rp, fname_save='../results/figs/perf_curves_individual.pdf', metric=metric, figsize=(6, 4.75), legend=False,\n",
    "                dset_names = ['financial_phrasebank', 'rotten_tomatoes', 'sst2', 'emotion'])\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rens = ravg\n",
    "rens = rens[rens.max_depth  == 8]\n",
    "rens = rens[rens.subsample_frac  == 1]\n",
    "for k in default_params:\n",
    "    rens = rens[rens[k] == default_params[k]]\n",
    "\n",
    "# groupings to plo\n",
    "groupings = 'model_name'\n",
    "# groupings = ['model_name', 'max_features', 'refinement_strategy', 'use_llm_prompt_context', 'ngrams']\n",
    "metric = 'roc_auc_test'\n",
    "# metric = 'accuracy_test'\n",
    "\n",
    "# viz.plot_train_and_test(rens, groupings, metric, x='n_estimators')\n",
    "fig = viz.plot_curves(rens, x='n_estimators', fname_save='../results/figs/perf_curves_ensemble.pdf', metric=metric, figsize=(6, 4.75), legend=False,\n",
    "                dset_names = ['financial_phrasebank', 'rotten_tomatoes', 'sst2', 'emotion'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from viz import *\n",
    "dvu.set_style()\n",
    "plt.figure(figsize=(6, 4.75))\n",
    "plt.rcParams['font.size'] = '14'\n",
    "x = 'n_estimators'\n",
    "y = 'roc_auc_test'\n",
    "rp = rens\n",
    "R, C = 2, 2\n",
    "# dset_names = rp['dataset_name'].unique()\n",
    "dset_names = ['financial_phrasebank', 'rotten_tomatoes', 'sst2', 'emotion']\n",
    "for i in range(R * C):\n",
    "    plt.subplot(R, C, i + 1)\n",
    "    dset_name = dset_names[i]\n",
    "    rd = rp[rp.dataset_name == dset_name] #.sort_values(by='model_name', ascending=False)\n",
    "    groupings = 'model_name'\n",
    "    rd = rd.sort_values(by=['model_name', 'n_estimators'], ascending=False)\n",
    "    for (k, g) in rd.groupby(by=groupings, sort=False):\n",
    "        # print(k)\n",
    "        if 'llm_tree' in k:\n",
    "            kwargs = {'lw': 2.5, 'alpha': 0.9, 'ls': '-', 'marker': '.', 'color': 'mediumseagreen', 'ms': 8}\n",
    "        else:\n",
    "            if 'decision_tree' in k:\n",
    "                color = '#BBB'\n",
    "            else:\n",
    "                color = '#111'\n",
    "            kwargs = {'alpha': 0.8, 'lw': 1.5, 'ls': '-', 'marker': '.', 'color': color, 'ms': 8}\n",
    "        # if i == 0:\n",
    "        kwargs['label'] = MODELS_RENAME_DICT.get(k, k)\n",
    "        if metric + '_err' in g.columns:\n",
    "            plt.errorbar(g[x], g[metric], yerr=g[metric + '_err'], **kwargs)\n",
    "        else:\n",
    "            plt.plot(g[x], g[metric], **kwargs)\n",
    "    plt.title(DSETS_RENAME_DICT.get(dset_name, dset_name), fontsize='large')\n",
    "    if i % 2 == 0:\n",
    "        plt.ylabel(f'ROC AUC', fontsize='medium')\n",
    "    if i >= 2:\n",
    "        plt.xlabel('# estimators', fontsize='large')\n",
    "    # plt.xscale('log')\n",
    "plt.tight_layout()\n",
    "plt.legend(fontsize='medium', bbox_to_anchor=(0.31, -0.45))\n",
    "\n",
    "plt.savefig('acc_ens.pdf', bbox_inches='tight')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subsampling curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensemble curves (note, only ran 1 seed for this)\n",
    "rens = ravg\n",
    "rens = rens[rens.max_depth  == 8]\n",
    "rens = rens[rens.n_estimators == 1]\n",
    "for k in default_params:\n",
    "    rens = rens[rens[k] == default_params[k]]\n",
    "\n",
    "# groupings to plo\n",
    "groupings = 'model_name'\n",
    "# groupings = ['model_name', 'max_features', 'refinement_strategy', 'use_llm_prompt_context', 'ngrams']\n",
    "metric = 'roc_auc'\n",
    "# metric = 'accuracy'\n",
    "\n",
    "# viz.plot_train_and_test(rens, groupings, metric, x='n_estimators')\n",
    "viz.plot_curves(rens, x='subsample_frac', fname_save='../results/figs/perf_curves_subsampling.pdf')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ablations table (with cross-validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply cross validation (selects best max_depth)\n",
    "d = ravg\n",
    "d.n_estimators == 1\n",
    "d.subsample_frac == 1\n",
    "groupings = ['model_name', 'max_features', 'ngrams',\n",
    "             'refinement_strategy', 'use_llm_prompt_context', 'use_stemming'] \n",
    "# ravg_cv = (\n",
    "#     d\n",
    "#     .sort_values(by='accuracy_cv', ascending=False)\n",
    "#     .groupby(by=groupings + ['dataset_name'])\n",
    "#     .first()  # selects best max_depth\n",
    "#     .reset_index()\n",
    "# )\n",
    "ravg_cv = ravg[ravg.max_depth == 12]\n",
    "\n",
    "# plt.figure(figsize=(8, 3))\n",
    "# sns.barplot(x='roc_auc_test', y=ravg_cv['dataset_name'].map(viz.DSETS_RENAME_DICT), hue='model_name', data=ravg_cv)\n",
    "# plt.xlim(left=0.5)\n",
    "# plt.ylabel('Dataset')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display all columns\n",
    "with pd.option_context('display.max_columns', None):\n",
    "    display(ravg_cv.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = 'roc_auc_test'\n",
    "# metric = 'accuracy_test'\n",
    "def round_3(x):\n",
    "    return x.apply(lambda x: f'{x:.3f}')\n",
    "ravg_cv['met_with_err'] = round_3(ravg_cv[metric]) + ' \\\\err{' + round_3(ravg_cv[metric+'_err']) + '}'\n",
    "# print(ravg_cv['met_with_err'])\n",
    "ablations = (\n",
    "    ravg_cv\n",
    "    .pivot_table(index=groupings, columns='dataset_name', values='met_with_err',\n",
    "                 aggfunc=lambda x: ' '.join(x)) # needed to allow for string values\n",
    "    .reset_index()\n",
    "    .rename_axis(None, axis=1)\n",
    ")\n",
    "# display(ablations)\n",
    "\n",
    "def rename_ablations(row):\n",
    "    tup = tuple(\n",
    "        row[groupings]\n",
    "              .values.tolist())\n",
    "    return {\n",
    "            ('decision_tree', 1, 2,  'llm', 0, 0): 'CART',\n",
    "            (          'id3', 1, 2,  'llm', 0, 0): 'ID3',\n",
    "            (     'llm_tree', 1, 2, 'embs', 0, 0): 'Aug-Tree (Embeddings)',\n",
    "            (     'llm_tree', 1, 2,  'llm', 0, 0): 'Aug-Tree',\n",
    "            (     'llm_tree', 1, 2,  'llm', 0, 1): 'Aug-Tree (Stemming)',\n",
    "            (     'llm_tree', 1, 2,  'llm', 1, 0): 'Aug-Tree (Contextual prompt)',\n",
    "            (     'llm_tree', 1, 3,  'llm', 0, 0): 'Aug-Tree (Trigrams)',\n",
    "            (     'llm_tree', 5, 2,  'llm', 0, 0): 'Aug-Tree (5 CART features)',\n",
    "    }[tup]\n",
    "    \n",
    "ablations.index = ablations.apply(\n",
    "    lambda x: rename_ablations(x), axis=1\n",
    ")\n",
    "ablations = ablations.drop(columns=groupings)\n",
    "ablations = ablations.reindex(['Aug-Tree', 'Aug-Tree (Embeddings)', 'Aug-Tree (Contextual prompt)', 'Aug-Tree (5 CART features)', 'Aug-Tree (Stemming)', 'Aug-Tree (Trigrams)', 'CART', 'ID3'])\n",
    "ablations = ablations.rename(index={'Aug-Tree': '\\\\textbf{Aug-Tree}'})\n",
    "ablations.iloc[0] = ablations.iloc[0].apply(lambda x: '\\\\textbf{' + str(x) + '}')\n",
    "\n",
    "print(\n",
    "    ablations\n",
    "    .rename(columns=viz.DSETS_RENAME_DICT)\n",
    "    .style\n",
    "    .format(precision=3)\n",
    "    .to_latex(hrules=True, ).replace('_', ' ')\n",
    "    .replace('\\nCART', '\\n\\\\midrule\\nCART')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix accuracy table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply cross validation (selects best max_depth)\n",
    "d_default = ravg\n",
    "for k in default_params:\n",
    "    d_default = d_default[d_default[k] == default_params[k]]\n",
    "d_default = d_default[d_default.subsample_frac == 1]\n",
    "d_default = d_default[d_default.model_name == 'llm_tree']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1672669/2995299158.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  d_single['met_with_err'] = round_3(d_single[metric]) + ' \\\\err{' + round_3(d_single[metric+'_err']) + '}'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>dataset_name</th>\n",
       "      <th>emotion</th>\n",
       "      <th>financial_phrasebank</th>\n",
       "      <th>rotten_tomatoes</th>\n",
       "      <th>sst2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>met_with_err</th>\n",
       "      <td>0.637 \\err{0.045}</td>\n",
       "      <td>0.818 \\err{0.014}</td>\n",
       "      <td>0.613 \\err{0.009}</td>\n",
       "      <td>0.571 \\err{0.018}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "dataset_name            emotion financial_phrasebank    rotten_tomatoes  \\\n",
       "met_with_err  0.637 \\err{0.045}    0.818 \\err{0.014}  0.613 \\err{0.009}   \n",
       "\n",
       "dataset_name               sst2  \n",
       "met_with_err  0.571 \\err{0.018}  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_single = d_default[d_default.max_depth == 12]\n",
    "metric = 'accuracy_test'\n",
    "d_single['met_with_err'] = round_3(d_single[metric]) + ' \\\\err{' + round_3(d_single[metric+'_err']) + '}'\n",
    "d_single[['dataset_name', 'met_with_err']].set_index('dataset_name').T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1672669/1333094728.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  d_ens_acc['met_with_err'] = round_3(d_ens_acc[metric]) + ' \\\\err{' + round_3(d_ens_acc[metric+'_err']) + '}'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>dataset_name</th>\n",
       "      <th>emotion</th>\n",
       "      <th>financial_phrasebank</th>\n",
       "      <th>rotten_tomatoes</th>\n",
       "      <th>sst2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>met_with_err</th>\n",
       "      <td>0.800 \\err{0.008}</td>\n",
       "      <td>0.848 \\err{0.006}</td>\n",
       "      <td>0.619 \\err{0.004}</td>\n",
       "      <td>0.614 \\err{0.016}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "dataset_name            emotion financial_phrasebank    rotten_tomatoes  \\\n",
       "met_with_err  0.800 \\err{0.008}    0.848 \\err{0.006}  0.619 \\err{0.004}   \n",
       "\n",
       "dataset_name               sst2  \n",
       "met_with_err  0.614 \\err{0.016}  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_ens_acc = d_default[d_default.n_estimators == 40]\n",
    "metric = 'accuracy_test'\n",
    "d_ens_acc['met_with_err'] = round_3(d_ens_acc[metric]) + ' \\\\err{' + round_3(d_ens_acc[metric+'_err']) + '}'\n",
    "d_ens_acc[['dataset_name', 'met_with_err']].set_index('dataset_name').T"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a9ff692d44ea03fd8a03facee7621117bbbb82def09bacaacf0a2cbc238b7b91"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
