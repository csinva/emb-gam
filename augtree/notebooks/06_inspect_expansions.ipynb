{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%config InlineBackend.figure_format = 'retina' # Make visualizations look good\n",
    "%matplotlib inline\n",
    "#%config InlineBackend.figure_format = 'svg' \n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from os.path import join\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "import dtreeviz\n",
    "import imodelsx.process_results\n",
    "from collections import defaultdict\n",
    "import sys\n",
    "import numpy as np\n",
    "import viz\n",
    "import llm_tree.llm\n",
    "import imodelsx\n",
    "import sklearn.tree\n",
    "from copy import deepcopy\n",
    "sys.path.append('../experiments/')\n",
    "results_dir = '../results/feb11/'\n",
    "\n",
    "def get_exansion_dfs(r):\n",
    "    dataset_names = list(r.dataset_name.unique())\n",
    "    dfs = []\n",
    "    for dataset_name in tqdm(dataset_names):\n",
    "\n",
    "        row = r[r.dataset_name == dataset_name].iloc[0]\n",
    "        model = pkl.load(open(join(row.save_dir_unique, 'model.pkl'), 'rb'))\n",
    "\n",
    "        d = defaultdict(list)\n",
    "        d_dict = defaultdict(list)\n",
    "        for i in range(len(model.estimators_)):\n",
    "            est = model.estimators_[i]\n",
    "            ks_list = list(est.get_tree_dict_repr().values())\n",
    "            d['keywords_list'] += ks_list\n",
    "            d['keyword'] += [ks[0] for ks in ks_list]\n",
    "            d['keyword_expanded'] += [ks[0] for ks in ks_list if len(ks) > 1]\n",
    "            for ks in ks_list:\n",
    "                d_dict[ks[0]].append(ks[1:])\n",
    "\n",
    "        # add value counts\n",
    "        df = pd.Series(d['keyword_expanded']).value_counts()[:20].reset_index().rename(\n",
    "            columns={\n",
    "                'index': 'Keyword',\n",
    "                0: '# Expansions',\n",
    "            }\n",
    "        )\n",
    "        df = df[~df['Keyword'].isin(STOPWORDS)]\n",
    "        def select_expand_with_median_length(k):\n",
    "            expands = d_dict[k]\n",
    "            lens = [len(e) for e in expands]\n",
    "            return expands[np.argsort(lens)[len(lens)//2]]\n",
    "        def select_expand_with_longest_length(k):\n",
    "            expands = d_dict[k]\n",
    "            lens = [len(e) for e in expands]\n",
    "            return expands[np.argsort(lens)[-1]]\n",
    "        df.insert(1, 'Example expansion',\n",
    "                df['Keyword'].apply(lambda k: ', '.join(select_expand_with_longest_length(k))))\n",
    "        df['Mean expansions'] = df['Keyword'].apply(lambda k: np.mean([len(e) for e in d_dict[k]]))\n",
    "        df['# Expansion candidates'] = df['Keyword'].apply(lambda k: len(EXPANSION_DICT[k]))\n",
    "        df.insert(0, 'Dataset', viz.DSETS_RENAME_DICT[dataset_name])\n",
    "        \n",
    "        dfs.append(df)\n",
    "    return dfs\n",
    "\n",
    "r = imodelsx.process_results.get_results_df(results_dir, use_cached=True)\n",
    "\n",
    "STOPWORDS = ['the', 'or', 'not', 'too', 'with', 'so', 'be',\n",
    "             'nt', 'it', 'this', 'and', 'so', 'that', 'are',\n",
    "             'said', 'from', 'per', 'the movie', 'movie']\n",
    "EXPANSION_DICT = pkl.load(open('/home/chansingh/llm-tree/experiments/gpt3_cache/base.pkl', 'rb'))\n",
    "EXPANSION_DICT_RAW = pkl.load(open('/home/chansingh/llm-tree/experiments/gpt3_cache/raw_base.pkl', 'rb'))\n",
    "r = r[r.n_estimators == 40]\n",
    "r = r[r.seed == 1]\n",
    "r = r[r.model_name == 'llm_tree']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expansions metadata table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_names = list(r.dataset_name.unique())\n",
    "dfs = []\n",
    "d = defaultdict(lambda: defaultdict(list))\n",
    "\n",
    "for dataset_name in tqdm(dataset_names):\n",
    "    row = r[r.dataset_name == dataset_name].iloc[0]\n",
    "    model = pkl.load(open(join(row.save_dir_unique, 'model.pkl'), 'rb'))\n",
    "\n",
    "    for i in range(len(model.estimators_)):\n",
    "        est = model.estimators_[i]\n",
    "        ks_list = list(est.get_tree_dict_repr().values())\n",
    "        for ks in ks_list:\n",
    "            keyword = ks[0]\n",
    "            expansion_candidates = llm_tree.llm.convert_response_to_keywords(EXPANSION_DICT_RAW[keyword], remove_duplicates=False)\n",
    "            d['\\makecell{# Expansion candidates\\\\\\\\(before deduplication)}'][keyword].append(len(expansion_candidates))\n",
    "            d['# Expansion candidates'][keyword].append(len(set(expansion_candidates)))\n",
    "            d['\\makecell{# Expansions\\\\\\\\(after screening)}'][keyword].append(len(ks[1:]))\n",
    "            # d['Fraction with any expansions'][keyword].append(1 if len(ks[1:]) > 1 else 0)\n",
    "\n",
    "tab = {}\n",
    "for k, v in d.items():\n",
    "    mu = np.mean([np.mean(lens) for lens in list(v.values())])\n",
    "    sem = np.std([np.mean(lens) for lens in list(v.values())]) / np.sqrt(len(v))\n",
    "    tab[k] = f'{mu:.1f}' + '\\err{' + f'{sem:.1f}' + '}'\n",
    "tab = pd.Series(tab).to_frame().T\n",
    "tab['\\makecell{Expansion relevance\\\\\\\\(human)}'] = '0.94\\err{0.3}'\n",
    "tab['\\makecell{Expansion relevance\\\\\\\\(randomized, human)}'] = '0.15\\err{0.4}'\n",
    "print(tab.round(2)\n",
    "      .to_latex(index=False, escape=False)\n",
    "      .replace('#', '\\#').replace('_', '\\_')\n",
    "      .replace('lllll', 'rrr|rr'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expansions example table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = get_exansion_dfs(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display df with long strings\n",
    "with pd.option_context('display.max_colwidth', 100, 'display.max_rows', None):\n",
    "    d_full = pd.concat(dfs).round(1)\n",
    "    d_full = d_full[d_full['Example expansion'].apply(lambda x: len(x) > 0)]\n",
    "    # d_full = d_full.astype(str).apply(lambda x: x.str[:100])\n",
    "    # d_full = d_full.astype(str)\n",
    "\n",
    "    # d_full = d_full.set_index('Dataset')\n",
    "    # drop rows that have duplicate Keyword\n",
    "    d_full = d_full.drop_duplicates(subset=['Keyword'])\n",
    "    d_full = d_full[d_full['# Expansions'] > 1]\n",
    "    d_full = d_full[d_full['Mean expansions'] > 0.5]\n",
    "    d_full = d_full.groupby('Dataset').head(6)\n",
    "\n",
    "    # replace repeat entries in dset with empty string\n",
    "    dset_counts = d_full['Dataset'].value_counts().to_dict()\n",
    "    dset = [''] * len(d_full)\n",
    "    dset[0] = dataset_names[0]\n",
    "    idx = 0\n",
    "    for i, dataset_name in enumerate(dataset_names):\n",
    "        dname = viz.DSETS_RENAME_DICT[dataset_names[i]]\n",
    "        count = dset_counts[viz.DSETS_RENAME_DICT[dataset_name]]\n",
    "        s = '\\\\parbox[c]{1mm}{\\\\multirow{' + str(count) + '}{*}{\\\\rotatebox[origin=c]{90} {' + dname + '}}}'\n",
    "        dset[idx] = s\n",
    "        idx += count\n",
    "    d_full['Dataset'] = dset \n",
    "    \n",
    "\n",
    "    display(d_full)\n",
    "\n",
    "    # display(d_full.style.hide(axis='index').to_latex(hrules=True))\n",
    "    with open('expansions.tex', 'w') as f:\n",
    "        s = d_full.to_latex(index=False, escape=False).replace('_', '\\_').replace('#', '\\#').replace('\\parbox', '\\midrule \\n \\parbox').replace(\"Dataset \", \"\")\n",
    "        f.write(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display df with long strings\n",
    "with pd.option_context('display.max_colwidth', 100, 'display.max_rows', None):\n",
    "    d_full = pd.concat(dfs).round(1)\n",
    "    d_full = d_full[d_full['Example expansion'].apply(lambda x: len(x) > 0)]\n",
    "    # d_full = d_full.astype(str).apply(lambda x: x.str[:100])\n",
    "    # d_full = d_full.astype(str)\n",
    "\n",
    "    # d_full = d_full.set_index('Dataset')\n",
    "    # drop rows that have duplicate Keyword\n",
    "    d_full = d_full.drop_duplicates(subset=['Keyword'])\n",
    "    d_full = d_full[d_full['# Expansions'] > 1]\n",
    "    d_full = d_full[d_full['Mean expansions'] > 0.5]\n",
    "    d_full = d_full.groupby('Dataset').head(2)\n",
    "\n",
    "    # replace repeat entries in dset with empty string\n",
    "    dset_counts = d_full['Dataset'].value_counts().to_dict()\n",
    "    dset = [''] * len(d_full)\n",
    "    dset[0] = dataset_names[0]\n",
    "    idx = 0\n",
    "    for i, dataset_name in enumerate(dataset_names):\n",
    "        dname = viz.DSETS_RENAME_DICT[dataset_names[i]]\n",
    "        count = dset_counts[viz.DSETS_RENAME_DICT[dataset_name]]\n",
    "        # s = '\\\\parbox[c]{1mm}{\\\\multirow{' + str(count) + '}{*}{\\\\rotatebox[origin=c]{90} {' + dname + '}}}'\n",
    "        s = viz.DSETS_RENAME_ABBREVIATE_DICT[dataset_name]\n",
    "        dset[idx] = s\n",
    "        idx += count\n",
    "    d_full['Dataset'] = dset \n",
    "    \n",
    "    d_full = d_full.drop(columns=['# Expansions', 'Mean expansions', '# Expansion candidates'])\n",
    "    display(d_full)\n",
    "\n",
    "    # display(d_full.style.hide(axis='index').to_latex(hrules=True))\n",
    "    with open('expansions.tex', 'w') as f:\n",
    "        s = d_full.to_latex(index=False, escape=False).replace('_', '\\_').replace('#', '\\#').replace('\\parbox', '\\midrule \\n \\parbox')\n",
    "        f.write(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export table for human scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = get_exansion_dfs(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_full = pd.concat(dfs).round(1)\n",
    "rng = np.random.default_rng(seed=1)\n",
    "\n",
    "nums = np.random.choice(50, size=30, replace=False)\n",
    "wrongs = defaultdict(list)\n",
    "for i in range(30//2):\n",
    "    wrongs['Keyword'].append(d_full.iloc[nums[2 * i]]['Keyword'])\n",
    "    wrongs['Example expansion'].append(d_full.iloc[nums[2 * i + 1]]['Example expansion'])\n",
    "d_wrongs = pd.DataFrame(wrongs)\n",
    "d_wrongs['Dataset'] = None\n",
    "\n",
    "d_study = pd.concat([d_full, d_wrongs])\n",
    "d_study['Wrongs'] = [0] * len(d_full) + [1] * len(d_wrongs)\n",
    "d_study = d_study.sample(frac=1, random_state=1)\n",
    "d_study = d_study[['Keyword', 'Example expansion', 'Wrongs', 'Dataset']]\n",
    "d_study = d_study[d_study['Example expansion'].apply(lambda x: len(x) > 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with pd.option_context('display.max_colwidth', 100, 'display.max_rows', None):\n",
    "    display(d_study)\n",
    "    d_study.to_csv('human_study.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_list = []\n",
    "for i in [1, 2, 3]:\n",
    "    ds = pd.read_csv(f'subj{i}.csv')\n",
    "    assert ds.shape[0] == d_study.shape[0]\n",
    "    ds['Dataset'] = d_study['Dataset'].fillna('Null').values\n",
    "    ds_list.append(deepcopy(ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = pd.concat(ds_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1599036/3133718405.py:1: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  ds.groupby('Dataset').mean()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Score (1 to 5)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dataset</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Emotion</th>\n",
       "      <td>4.380952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Financial phrasebank</th>\n",
       "      <td>4.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Null</th>\n",
       "      <td>1.311111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rotten tomatoes</th>\n",
       "      <td>4.433333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SST2</th>\n",
       "      <td>4.571429</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Score (1 to 5)\n",
       "Dataset                             \n",
       "Emotion                     4.380952\n",
       "Financial phrasebank        4.166667\n",
       "Null                        1.311111\n",
       "Rotten tomatoes             4.433333\n",
       "SST2                        4.571429"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.groupby('Dataset').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1599036/2631380902.py:1: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sem is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  ds.groupby('Dataset').sem()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Score (1 to 5)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dataset</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Emotion</th>\n",
       "      <td>0.159676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Financial phrasebank</th>\n",
       "      <td>0.161835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Null</th>\n",
       "      <td>0.099606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rotten tomatoes</th>\n",
       "      <td>0.132902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SST2</th>\n",
       "      <td>0.103048</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Score (1 to 5)\n",
       "Dataset                             \n",
       "Emotion                     0.159676\n",
       "Financial phrasebank        0.161835\n",
       "Null                        0.099606\n",
       "Rotten tomatoes             0.132902\n",
       "SST2                        0.103048"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.groupby('Dataset').sem()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a9ff692d44ea03fd8a03facee7621117bbbb82def09bacaacf0a2cbc238b7b91"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
