{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "import sys\n",
    "import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "sys.path.append('../experiments/')\n",
    "import os\n",
    "import scipy\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import numpy as np\n",
    "import transformers\n",
    "import sys\n",
    "from os.path import join\n",
    "import datasets\n",
    "from dict_hash import sha256\n",
    "import numpy as np\n",
    "from torch.autograd import grad\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from torch.autograd.functional import jacobian\n",
    "from torch.func import jacfwd\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchhd\n",
    "from data import BabyLMDataset, BABYLM_ROOT_DIR\n",
    "from lm import HDLM\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = 'gpt2'\n",
    "tok = AutoTokenizer.from_pretrained(checkpoint)\n",
    "start = 'Roses are red, violets are'\n",
    "end = ' blue'\n",
    "lm = HDLM(checkpoint, device='cpu', emb_size=10000, learning_rate=0.1)\n",
    "\n",
    "print('tokens as strings', lm.tokenizer.tokenize(start))\n",
    "input_ids = lm.tokenizer.encode(start, return_tensors='pt').to(lm.device)\n",
    "print('tokens as ids', input_ids)\n",
    "\n",
    "lm.get_embs(input_ids).shape\n",
    "\n",
    "next_emb = lm.next_emb_from_token_ids(input_ids)\n",
    "print('next_emb', next_emb.shape)\n",
    "\n",
    "next_token_id = lm.emb_to_token_id(next_emb)\n",
    "print('next_token original', next_token_id, lm.tokenizer.decode(next_token_id))\n",
    "\n",
    "# update vocab emb\n",
    "next_token_correct_id = lm.tokenizer(end)['input_ids']\n",
    "lm.update_vocab_emb(next_emb, next_token_correct_id)\n",
    "\n",
    "\n",
    "next_token_id = lm.emb_to_token_id(next_emb)\n",
    "print('next_token', next_token_id, lm.tokenizer.decode(next_token_id))\n",
    "assert lm.tokenizer.decode(next_token_id) == end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = 'gpt2'\n",
    "device = 'cuda'\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "context_length = 3\n",
    "learning_rate = 0.5  # 0.01\n",
    "\n",
    "# initialize\n",
    "lm = HDLM(checkpoint, device=device, emb_size=50000,\n",
    "          learning_rate=learning_rate, context_length=context_length)\n",
    "dset = BabyLMDataset(join(BABYLM_ROOT_DIR, 'babylm_dev',\n",
    "                     'full.joblib'), context_length)\n",
    "dset_test = BabyLMDataset(\n",
    "    join(BABYLM_ROOT_DIR, 'babylm_test', 'full.joblib'), context_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_idx = 0\n",
    "d = defaultdict(list)\n",
    "test_kwargs = dict(dset=dset_test, n_examples=200, train=False, seed=13)\n",
    "train_kwargs = dict(dset=dset, n_examples=200, train=True)\n",
    "for batch_num in tqdm(range(100)):\n",
    "\n",
    "    # test\n",
    "    perplexity_test = lm.calc_perplexity(**test_kwargs)\n",
    "    d['perplexity_test'].append(perplexity_test)\n",
    "    d['batch_num'].append(batch_num)\n",
    "\n",
    "    # train\n",
    "    perplexity_train = lm.calc_perplexity(**train_kwargs, seed=0)\n",
    "\n",
    "    print('perplexity_test', perplexity_test)\n",
    "    print('perplexity_train', perplexity_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run an example\n",
    "toks, next = dset[0]\n",
    "print(repr(lm.tokenizer.decode(toks)), '->', repr(lm.tokenizer.decode(next)))\n",
    "emb_pred = lm.next_emb_from_token_ids(toks)\n",
    "tok_pred = lm.tokenizer.decode(lm.emb_to_token_id(emb_pred))\n",
    "print('pred', repr(tok_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a9ff692d44ea03fd8a03facee7621117bbbb82def09bacaacf0a2cbc238b7b91"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
