{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "import sys\n",
    "import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "sys.path.append('../experiments/')\n",
    "import os\n",
    "import scipy\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import numpy as np\n",
    "import transformers\n",
    "import sys\n",
    "from os.path import join\n",
    "import datasets\n",
    "from dict_hash import sha256\n",
    "import numpy as np\n",
    "from torch.autograd import grad\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from torch.autograd.functional import jacobian\n",
    "from torch.func import jacfwd\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchhd\n",
    "from data import BabyLMDataset, BABYLM_ROOT_DIR\n",
    "from lm import HDLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens as strings ['R', 'oses', 'Ġare', 'Ġred', ',', 'Ġv', 'io', 'lets', 'Ġare']\n",
      "tokens as ids tensor([[  49, 4629,  389, 2266,   11,  410,  952, 5289,  389]])\n",
      "next_emb torch.Size([10000])\n",
      "next_token tensor(47514) Bah\n",
      "next_token tensor(4171)  blue\n"
     ]
    }
   ],
   "source": [
    "checkpoint = 'gpt2'\n",
    "tok = AutoTokenizer.from_pretrained(checkpoint)\n",
    "start = 'Roses are red, violets are'\n",
    "end = ' blue'\n",
    "lm = HDLM(checkpoint, device='cpu', emb_size=10000, learning_rate=0.1)\n",
    "\n",
    "print('tokens as strings', lm.tokenizer.tokenize(start))\n",
    "input_ids = lm.tokenizer.encode(start, return_tensors='pt').to(lm.device)\n",
    "print('tokens as ids', input_ids)\n",
    "\n",
    "lm.get_embs(input_ids).shape\n",
    "\n",
    "next_emb = lm.next_emb_from_token_ids(input_ids)\n",
    "print('next_emb', next_emb.shape)\n",
    "\n",
    "next_token_id = lm.emb_to_token_id(next_emb)\n",
    "print('next_token', next_token_id, lm.tokenizer.decode(next_token_id))\n",
    "\n",
    "# update vocab emb\n",
    "next_token_correct_id = lm.tokenizer(end)['input_ids']\n",
    "lm.update_vocab_emb(next_emb, next_token_correct_id)\n",
    "\n",
    "\n",
    "next_token_id = lm.emb_to_token_id(next_emb)\n",
    "print('next_token', next_token_id, lm.tokenizer.decode(next_token_id))\n",
    "assert lm.tokenizer.decode(next_token_id) == end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = 'gpt2'\n",
    "device = 'cuda'\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "start = 'Roses are red, violets are'\n",
    "end = ' blue'\n",
    "lm = HDLM(checkpoint, device='cpu', emb_size=10000, learning_rate=0.1)\n",
    "dset = BabyLMDataset(join(BABYLM_ROOT_DIR, 'babylm_dev', 'full.joblib'), 10)\n",
    "dset_test = BabyLMDataset(\n",
    "    join(BABYLM_ROOT_DIR, 'babylm_test', 'full.joblib'), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:21<00:00,  4.64it/s]\n",
      "100%|██████████| 100/100 [00:46<00:00,  2.14it/s]\n"
     ]
    }
   ],
   "source": [
    "perplexity = lm.calc_perplexity(dset, n_examples=100, train=False)\n",
    "perplexity_test = lm.calc_perplexity(dset_test, n_examples=100, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "perp 10.765946388244629 perp test 10.876726150512695\n"
     ]
    }
   ],
   "source": [
    "print('perp', perplexity, 'perp test', perplexity_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:22<00:00,  4.36it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "17.713855743408203"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.calc_perplexity(dset, n_examples=100, train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:25<00:00,  3.99it/s]\n",
      "100%|██████████| 100/100 [00:48<00:00,  2.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "perp 21.905921936035156 perp test 39.63344955444336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "perplexity = lm.calc_perplexity(dset, n_examples=100, train=False)\n",
    "perplexity_test = lm.calc_perplexity(dset_test, n_examples=100, train=False)\n",
    "print('perp', perplexity, 'perp test', perplexity_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Doilies, Pam? \\nHello, Br' -> 'idget'\n",
      "pred '.'\n"
     ]
    }
   ],
   "source": [
    "# run an example\n",
    "toks, next = dset[0]\n",
    "print(repr(lm.tokenizer.decode(toks)), '->', repr(lm.tokenizer.decode(next)))\n",
    "emb_pred = lm.next_emb_from_token_ids(toks)\n",
    "tok_pred = lm.tokenizer.decode(lm.emb_to_token_id(emb_pred))\n",
    "print('pred', repr(tok_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a9ff692d44ea03fd8a03facee7621117bbbb82def09bacaacf0a2cbc238b7b91"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
