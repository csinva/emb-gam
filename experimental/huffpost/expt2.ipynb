{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "from imodelsx import embgam\n",
    "from datasets import load_dataset\n",
    "from sklearn.base import ClassifierMixin, RegressorMixin\n",
    "import pandas as pd\n",
    "from preprocess import clean_headlines, sample_data\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle as pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, X, warn=True):\n",
    "    \"\"\"For regression returns continuous output.\n",
    "    For classification, returns discrete output.\n",
    "    \"\"\"\n",
    "    preds = _predict_cached(model, X, warn=warn)\n",
    "    if isinstance(model, RegressorMixin):\n",
    "        return preds\n",
    "    elif isinstance(model, ClassifierMixin):\n",
    "        if preds.ndim > 1:  # multiclass classification\n",
    "            return np.argmax(preds, axis=1)\n",
    "        return ((preds + model.linear.intercept_) > 0).astype(int)\n",
    "\n",
    "\n",
    "def _predict_cached(model, X, warn):\n",
    "    \"\"\"Predict only the cached coefs in model.coefs_dict_\"\"\"\n",
    "    assert hasattr(model, \"coefs_dict_\"), \"coefs are not cached!\"\n",
    "    preds = []\n",
    "    n_unseen_ngrams = 0\n",
    "    for x in X:\n",
    "        pred = np.zeros(len(model.classes_))\n",
    "        seqs = embgam.embed.generate_ngrams_list(\n",
    "            x,\n",
    "            ngrams=model.ngrams,\n",
    "            tokenizer_ngrams=model.tokenizer_ngrams,\n",
    "            all_ngrams=model.all_ngrams,\n",
    "        )\n",
    "        for seq in seqs:\n",
    "            if seq in model.coefs_dict_:\n",
    "                pred += model.coefs_dict_[seq]\n",
    "            else:\n",
    "                n_unseen_ngrams += 1\n",
    "        preds.append(pred)\n",
    "\n",
    "    return np.array(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmodels/huffpost_\u001b[39m\u001b[39m{\u001b[39;00mtrain_year\u001b[39m}\u001b[39;00m\u001b[39m_embgam_ngrams=2.pkl\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m     17\u001b[0m     m \u001b[39m=\u001b[39m pkl\u001b[39m.\u001b[39mload(f)\n\u001b[0;32m---> 18\u001b[0m preds \u001b[39m=\u001b[39m predict(m, test_data)\n\u001b[1;32m     19\u001b[0m res[i, j] \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmean(preds \u001b[39m==\u001b[39m test_labels)\n\u001b[1;32m     20\u001b[0m std[i, j] \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "Cell \u001b[0;32mIn[2], line 5\u001b[0m, in \u001b[0;36mpredict\u001b[0;34m(model, X, warn)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict\u001b[39m(model, X, warn\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[1;32m      2\u001b[0m     \u001b[39m\"\"\"For regression returns continuous output.\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39m    For classification, returns discrete output.\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m     preds \u001b[39m=\u001b[39m _predict_cached(model, X, warn\u001b[39m=\u001b[39;49mwarn)\n\u001b[1;32m      6\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(model, RegressorMixin):\n\u001b[1;32m      7\u001b[0m         \u001b[39mreturn\u001b[39;00m preds\n",
      "Cell \u001b[0;32mIn[2], line 21\u001b[0m, in \u001b[0;36m_predict_cached\u001b[0;34m(model, X, warn)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m X:\n\u001b[1;32m     20\u001b[0m     pred \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros(\u001b[39mlen\u001b[39m(model\u001b[39m.\u001b[39mclasses_))\n\u001b[0;32m---> 21\u001b[0m     seqs \u001b[39m=\u001b[39m embgam\u001b[39m.\u001b[39;49membed\u001b[39m.\u001b[39;49mgenerate_ngrams_list(\n\u001b[1;32m     22\u001b[0m         x,\n\u001b[1;32m     23\u001b[0m         ngrams\u001b[39m=\u001b[39;49mmodel\u001b[39m.\u001b[39;49mngrams,\n\u001b[1;32m     24\u001b[0m         tokenizer_ngrams\u001b[39m=\u001b[39;49mmodel\u001b[39m.\u001b[39;49mtokenizer_ngrams,\n\u001b[1;32m     25\u001b[0m         all_ngrams\u001b[39m=\u001b[39;49mmodel\u001b[39m.\u001b[39;49mall_ngrams,\n\u001b[1;32m     26\u001b[0m     )\n\u001b[1;32m     27\u001b[0m     \u001b[39mfor\u001b[39;00m seq \u001b[39min\u001b[39;00m seqs:\n\u001b[1;32m     28\u001b[0m         \u001b[39mif\u001b[39;00m seq \u001b[39min\u001b[39;00m model\u001b[39m.\u001b[39mcoefs_dict_:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.7/envs/emb-gam/lib/python3.9/site-packages/imodelsx/embgam/embed.py:33\u001b[0m, in \u001b[0;36mgenerate_ngrams_list\u001b[0;34m(sentence, ngrams, tokenizer_ngrams, all_ngrams, parsing, nlp_chunks)\u001b[0m\n\u001b[1;32m     29\u001b[0m     seqs \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m [\u001b[39mstr\u001b[39m(x) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m tokenizer_ngrams(sentence)]\n\u001b[1;32m     31\u001b[0m \u001b[39m# all ngrams in loop\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 33\u001b[0m     unigrams_list \u001b[39m=\u001b[39m [x \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m tokenizer_ngrams(sentence)]\n\u001b[1;32m     34\u001b[0m     \u001b[39mif\u001b[39;00m all_ngrams:\n\u001b[1;32m     35\u001b[0m         ngram_lengths \u001b[39m=\u001b[39m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, ngrams \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.7/envs/emb-gam/lib/python3.9/site-packages/spacy/tokenizer.pyx:155\u001b[0m, in \u001b[0;36mspacy.tokenizer.Tokenizer.__call__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.7/envs/emb-gam/lib/python3.9/site-packages/spacy/tokenizer.pyx:191\u001b[0m, in \u001b[0;36mspacy.tokenizer.Tokenizer._tokenize_affixes\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.7/envs/emb-gam/lib/python3.9/site-packages/spacy/tokenizer.pyx:395\u001b[0m, in \u001b[0;36mspacy.tokenizer.Tokenizer._tokenize\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.7/envs/emb-gam/lib/python3.9/site-packages/spacy/tokenizer.pyx:473\u001b[0m, in \u001b[0;36mspacy.tokenizer.Tokenizer._attach_tokens\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.7/envs/emb-gam/lib/python3.9/site-packages/spacy/vocab.pyx:160\u001b[0m, in \u001b[0;36mspacy.vocab.Vocab.get\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.7/envs/emb-gam/lib/python3.9/site-packages/spacy/vocab.pyx:197\u001b[0m, in \u001b[0;36mspacy.vocab.Vocab._new_lexeme\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.7/envs/emb-gam/lib/python3.9/site-packages/spacy/lang/lex_attrs.py:105\u001b[0m, in \u001b[0;36mlike_url\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(text)):\n\u001b[1;32m    104\u001b[0m     \u001b[39mif\u001b[39;00m text[i] \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 105\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m    106\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    107\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# load data\n",
    "with open(f\"Data/huffpost.pkl\", \"rb\") as f:\n",
    "    huffpost_data = pkl.load(f)\n",
    "\n",
    "huffpost_data = clean_headlines(huffpost_data)\n",
    "\n",
    "# load model\n",
    "years = [2012, 2013, 2014, 2015, 2016, 2017, 2018]\n",
    "res = np.zeros((len(years), len(years)))\n",
    "std = np.zeros((len(years), len(years)))\n",
    "for i, train_year in enumerate(years):\n",
    "    for j, test_year in enumerate(years):\n",
    "        test_data, test_labels = sample_data(huffpost_data, year=test_year, in_dist=False, frac=1)\n",
    "\n",
    "        if train_year == test_year:\n",
    "            with open(f\"models/huffpost_{train_year}_embgam_ngrams=2.pkl\", \"rb\") as f:\n",
    "                m = pkl.load(f)\n",
    "            preds = predict(m, test_data)\n",
    "            res[i, j] = np.mean(preds == test_labels)\n",
    "            std[i, j] = 0\n",
    "\n",
    "        else:\n",
    "            try:\n",
    "                acc = []\n",
    "                for seed in [42, 192, 852, 5555]:\n",
    "                    with open(f\"models/expt1/expt1_train_{train_year}_test_{test_year}_seed_{seed}.pkl\", \"rb\") as f:\n",
    "                        m = pkl.load(f)\n",
    "\n",
    "                    # predict\n",
    "                    preds = predict(m, test_data)\n",
    "                    acc.append(np.mean(preds == test_labels))\n",
    "                res[i,j] = np.mean(acc)\n",
    "                std[i,j] = np.std(acc)\n",
    "            except:\n",
    "                print(f\"No data for train year {train_year} and test year {test_year}\")\n",
    "                continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('emb-gam')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8261e6e9195907ef1b1bb27e02e12314814141bbdf3682fed8f6dc8c27be7d37"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
